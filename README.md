# GPT-2 Question Answering with Transformers

This project utilizes OpenAI's GPT-2 model for question-answering based on a given content input. It employs the `transformers` library by Hugging Face to load a pre-trained GPT-2 model and generate responses.

## ðŸš€ Features

- Uses GPT-2 for generating answers to user questions.  
- Supports text-based prompts for custom knowledge retrieval.  
- Implements tokenization and text generation techniques like top-k sampling, nucleus sampling, and temperature control.  
- Returns answers in a structured JSON format.  

## ðŸ“Œ Installation

Make sure you have Python 3.7+ installed.
```
pip install -r requirements.txt
```
